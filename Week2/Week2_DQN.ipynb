{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca209422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import pygame\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN model which takes in the state as an input and outputs predicted q values for every possible action\n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super().__init__()\n",
    "        # Add your architecture parameters here\n",
    "        # You can use nn.Functional\n",
    "        # Remember that the input is of size batch_size x state_space\n",
    "        # and the output is of size batch_size x action_space (ulta ho sakta hai dekh lo)\n",
    "        # TODO: Add code here\n",
    "        pass\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: Complete based on your implementation\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While training neural networks, we split the data into batches.\n",
    "# To improve the training, we need to remove the \"correlation\" between game states\n",
    "# The buffer starts storing states and once it reaches maximum capacity, it replaces\n",
    "# states at random which reduces the correlation.\n",
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe4d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training logic for CartPole environment here\n",
    "# Remember to use the ExperienceBuffer and a target network\n",
    "# Details can be found in the book sent in the group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6493553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cartpole_model(model, episodes=10, render=True):\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation for cartpole here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnakeGame(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 10}\n",
    "\n",
    "    def __init__(self, size=10, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.cell_size = 30\n",
    "        self.screen_size = self.size * self.cell_size\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)  # 0: right, 1: up, 2: left, 3: down\n",
    "        self.observation_space = gym.spaces.Box(0, 2, shape=(self.size, self.size), dtype=np.uint8)\n",
    "\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "\n",
    "        self.snake = deque()\n",
    "        self.food = None\n",
    "        self.direction = [1, 0]\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.snake.clear()\n",
    "        mid = self.size // 2\n",
    "        self.snake.appendleft([mid, mid])\n",
    "        self.direction = [1, 0]\n",
    "        self._place_food()\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_init()\n",
    "\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: Change reward schema to avoid the following\n",
    "        # 1) 180 degree turns\n",
    "        # 2) Wall collisions\n",
    "        # 3) Being slow at collecting food\n",
    "\n",
    "        if action == 0 and self.direction != [-1, 0]: self.direction = [1, 0]\n",
    "        elif action == 1 and self.direction != [0, 1]: self.direction = [0, -1]\n",
    "        elif action == 2 and self.direction != [1, 0]: self.direction = [-1, 0]\n",
    "        elif action == 3 and self.direction != [0, -1]: self.direction = [0, 1]\n",
    "\n",
    "        head = self.snake[0]\n",
    "        new_head = [head[0] + self.direction[0], head[1] + self.direction[1]]\n",
    "\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        if not (0 <= new_head[0] < self.size and 0 <= new_head[1] < self.size):\n",
    "            done = True\n",
    "        else:\n",
    "            body_to_check = list(self.snake)[:-1] if new_head != self.food else list(self.snake)\n",
    "            if new_head in body_to_check:\n",
    "                done = True\n",
    "\n",
    "        if not done:\n",
    "            self.snake.appendleft(new_head)\n",
    "            if new_head == self.food:\n",
    "                self._place_food()\n",
    "            else:\n",
    "                self.snake.pop()\n",
    "\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        return obs, reward, done, False, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # TODO: Return an observation state, take inspiration from the observation_space attribute\n",
    "        pass\n",
    "\n",
    "    def _place_food(self):\n",
    "        positions = set(tuple(p) for p in self.snake)\n",
    "        empty = [(x, y) for x in range(self.size) for y in range(self.size) if (x, y) not in positions]\n",
    "        self.food = list(random.choice(empty)) if empty else None\n",
    "\n",
    "    def render(self):\n",
    "        if self.screen is None:\n",
    "            self._render_init()\n",
    "\n",
    "        self.screen.fill((0, 0, 0))\n",
    "        for x, y in self.snake:\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (0, 255, 0),\n",
    "                pygame.Rect(x * self.cell_size, y * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "        if self.food:\n",
    "            fx, fy = self.food\n",
    "            pygame.draw.rect(\n",
    "                self.screen, (255, 0, 0),\n",
    "                pygame.Rect(fx * self.cell_size, fy * self.cell_size, self.cell_size, self.cell_size)\n",
    "            )\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def _render_init(self):\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode((self.size * self.cell_size, self.size * self.cell_size))\n",
    "        self.clock = pygame.time.Clock()\n",
    "\n",
    "    def close(self):\n",
    "        if self.screen:\n",
    "            pygame.quit()\n",
    "            self.screen = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ffb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training logic for Snake Game here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e10565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_snake_model(model, size=20, episodes=10, render=True):\n",
    "    env = SnakeGame(size=size, render_mode=\"human\" if render else None)\n",
    "    model.eval()\n",
    "\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        obs, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            state = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = model(state)\n",
    "                action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            if render:\n",
    "                env.render()\n",
    "    \n",
    "        rewards.append(total_reward)\n",
    "        print(f\"Episode {episode + 1}: Reward = {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "    avg_reward = sum(rewards) / episodes\n",
    "\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7eb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run evaluation for Snake Game here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChaseEscapeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dt = 0.1\n",
    "        self.max_speed = 0.4\n",
    "        self.agent_radius = 0.05\n",
    "        self.target_radius = 0.05\n",
    "        self.chaser_radius = 0.07\n",
    "        self.chaser_speed = 0.03\n",
    "\n",
    "        self.action_space = gym.spaces.MultiDiscrete([3, 3])  # actions in {0,1,2} map to [-1,0,1]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(8,),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.screen_size = 500\n",
    "        self.np_random = None\n",
    "\n",
    "        if render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "    def sample_pos(self, far_from=None, min_dist=0.5):\n",
    "        while True:\n",
    "            pos = self.np_random.uniform(low=-0.8, high=0.8, size=(2,))\n",
    "            if far_from is None or np.linalg.norm(pos - far_from) >= min_dist:\n",
    "                return pos\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.agent_pos = self.sample_pos()\n",
    "        self.agent_vel = np.zeros(2, dtype=np.float32)\n",
    "        self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "        self.chaser_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.7)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # TODO: Decide how to pass the state (don't use pixel values)\n",
    "        pass\n",
    "\n",
    "    def _get_info(self):\n",
    "        return {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # TODO: Add reward scheme\n",
    "        # 1) Try to make the agent stay within bounds\n",
    "        # 2) The agent shouldn't idle around\n",
    "        # 3) The agent should go for the reward\n",
    "        # 4) The agent should avoid the chaser\n",
    "\n",
    "        accel = (np.array(action) - 1) * 0.1\n",
    "        self.agent_vel += accel\n",
    "        self.agent_vel = np.clip(self.agent_vel, -self.max_speed, self.max_speed)\n",
    "        self.agent_pos += self.agent_vel * self.dt\n",
    "        self.agent_pos = np.clip(self.agent_pos, -1, 1)\n",
    "\n",
    "        direction = self.agent_pos - self.chaser_pos\n",
    "        norm = np.linalg.norm(direction)\n",
    "        if norm > 1e-5:\n",
    "            self.chaser_pos += self.chaser_speed * direction / norm\n",
    "\n",
    "        dist_to_target = np.linalg.norm(self.agent_pos - self.target_pos)\n",
    "        dist_to_chaser = np.linalg.norm(self.agent_pos - self.chaser_pos)\n",
    "\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "\n",
    "        if dist_to_target < self.agent_radius + self.target_radius:\n",
    "            self.target_pos = self.sample_pos(far_from=self.agent_pos, min_dist=0.5)\n",
    "\n",
    "        if dist_to_chaser < self.agent_radius + self.chaser_radius:\n",
    "            terminated = True\n",
    "\n",
    "        return self._get_obs(), reward, terminated, False, self._get_info()\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.close()\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        def to_screen(p):\n",
    "            x = int((p[0] + 1) / 2 * self.screen_size)\n",
    "            y = int((1 - (p[1] + 1) / 2) * self.screen_size)\n",
    "            return x, y\n",
    "\n",
    "        pygame.draw.circle(self.screen, (0, 255, 0), to_screen(self.target_pos), int(self.target_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (0, 0, 255), to_screen(self.agent_pos), int(self.agent_radius * self.screen_size))\n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), to_screen(self.chaser_pos), int(self.chaser_radius * self.screen_size))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(self.metadata[\"render_fps\"])\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == \"human\":\n",
    "            pygame.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train and evaluate CatMouseEnv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
